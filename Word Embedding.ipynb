{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary libraries\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the words that appear in the wordsim file\n",
    "wordsim = set()\n",
    "with open(\"./combined.csv\", \"rt\") as inf:\n",
    "    for line in inf.readlines():\n",
    "        ws = line.split(',')\n",
    "        wordsim.add(ws[0])\n",
    "        wordsim.add(ws[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10411221"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the contents of all the books\n",
    "al = \"\"\n",
    "for bookname in os.listdir('./books/'):\n",
    "    with open(\"./books/\" + bookname) as book:\n",
    "        text = book.readlines()\n",
    "        text = \" \".join(text)\n",
    "    al += text\n",
    "    \n",
    "len(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all unnecessary words and whitespaces\n",
    "al = re.sub('[0-9]+', '', al)\n",
    "al = re.sub('[\\s]{2,}', ' ', al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words from the books\n",
    "cv = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "result = cv.fit_transform([al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with the word counts\n",
    "cnt = defaultdict(int)\n",
    "lst = cv.get_feature_names()\n",
    "stpw = set(stopwords.words('english'))\n",
    "\n",
    "for i, w in enumerate(lst):\n",
    "    word = w.replace('_', '')\n",
    "    if word not in stpw and len(word) > 2:\n",
    "        cnt[word] += result.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10000 most frequent words \n",
    "sorted_counts = dict(sorted(cnt.items(), key=lambda item: item[1], reverse=True))\n",
    "chosen_words = set(list(sorted_counts.keys())[:10000])\n",
    "\n",
    "# add the remaining words that appear in both\n",
    "# the books and wordsim\n",
    "for w in sorted_counts.keys():\n",
    "    if w in wordsim:\n",
    "        chosen_words.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg20897.txt\n",
      "28338-0.txt\n",
      "pg14558.txt\n",
      "pg22277.txt\n",
      "6138-0.txt\n",
      "pg40498.txt\n",
      "pg9799.txt\n",
      "pg40643.txt\n",
      "pg20816.txt\n",
      "pg34.txt\n",
      "pg6679.txt\n",
      "pg1612.txt\n",
      "pg33962.txt\n",
      "pg22002.txt\n",
      "pg42069.txt\n",
      "47367-0.txt\n",
      "5352-0.txt\n",
      "7843-0.txt\n",
      "47436-0.txt\n",
      "pg48344.txt\n"
     ]
    }
   ],
   "source": [
    "# parse each book\n",
    "WINDOW_SIZE = 2\n",
    "illegal_chars = \".,!?/<>{}[]()\\\\|-+_=#@&*\\\"\\'\"\n",
    "pairs = set()\n",
    "\n",
    "for bookname in os.listdir('./books/'):\n",
    "    print(bookname)\n",
    "    with open(\"./books/\" + bookname) as book:\n",
    "        text = book.readlines()\n",
    "        text = \" \".join(text)\n",
    "        text = text.lower()\n",
    "    # tokenize text by sentences\n",
    "    for sentence in sent_tokenize(text):\n",
    "        # tokenize sentence by words\n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        # iterate through words\n",
    "        for i, word in enumerate(words):\n",
    "            # exclude word if it is an illegal character\n",
    "            if word in illegal_chars:\n",
    "                continue\n",
    "            \n",
    "            # define lower and upper bounds of a words environment\n",
    "            lower = max(i - WINDOW_SIZE, 0)\n",
    "            upper = min(i + WINDOW_SIZE, len(words))\n",
    "            \n",
    "            # create wordpairs \n",
    "            for j in range(lower, upper):\n",
    "                if words[j] == word or words[j] in illegal_chars:\n",
    "                    continue\n",
    "                pairs.add((word, words[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1376492"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those pairs where both words appear in wordsim\n",
    "train_data = list(filter(lambda x: x[0] in chosen_words and x[1] in chosen_words, pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82312"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset generator class\n",
    "class EmbeddingSet(Dataset):\n",
    "    def __init__(self, pairs, word_list):\n",
    "        self.pairs = pairs\n",
    "        self.word_list = word_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        w1, w2 = self.pairs[idx]\n",
    "        \n",
    "        idx1 = self.word_list.index(w1)\n",
    "        idx2 = self.word_list.index(w2)\n",
    "        \n",
    "        ohe_x = np.zeros((len(self.word_list),))\n",
    "        ohe_y = np.zeros((len(self.word_list),))\n",
    "        ohe_x[idx1] = 1\n",
    "        ohe_x = ohe_x.astype('float32')\n",
    "        \n",
    "        return torch.tensor(ohe_x), torch.tensor(idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "dataset = EmbeddingSet(train_data, list(chosen_words))\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN model class\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features, 100)\n",
    "        self.linear2 = nn.Linear(100, in_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]       1,026,900\n",
      "            Linear-2                [-1, 10268]       1,037,068\n",
      "================================================================\n",
      "Total params: 2,063,968\n",
      "Trainable params: 2,063,968\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 0.08\n",
      "Params size (MB): 7.87\n",
      "Estimated Total Size (MB): 7.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print summary of model\n",
    "model = EmbeddingModel(len(chosen_words))\n",
    "summary(model.cuda(), (len(chosen_words),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (linear1): Linear(in_features=10268, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=10268, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define necessary parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "log_interval = 50\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Batches: 0/824. Loss: 9.243396759033203.\n",
      "Epoch 0. Batches: 50/824. Loss: 9.083513259887695.\n",
      "Epoch 0. Batches: 100/824. Loss: 8.280298233032227.\n",
      "Epoch 0. Batches: 150/824. Loss: 7.586933612823486.\n",
      "Epoch 0. Batches: 200/824. Loss: 7.8843512535095215.\n",
      "Epoch 0. Batches: 250/824. Loss: 7.7210693359375.\n",
      "Epoch 0. Batches: 300/824. Loss: 7.8387370109558105.\n",
      "Epoch 0. Batches: 350/824. Loss: 7.501501083374023.\n",
      "Epoch 0. Batches: 400/824. Loss: 7.869247913360596.\n",
      "Epoch 0. Batches: 450/824. Loss: 7.652331352233887.\n",
      "Epoch 0. Batches: 500/824. Loss: 7.494274139404297.\n",
      "Epoch 0. Batches: 550/824. Loss: 7.701819896697998.\n",
      "Epoch 0. Batches: 600/824. Loss: 7.466142177581787.\n",
      "Epoch 0. Batches: 650/824. Loss: 7.3699798583984375.\n",
      "Epoch 0. Batches: 700/824. Loss: 7.584627628326416.\n",
      "Epoch 0. Batches: 750/824. Loss: 7.120809555053711.\n",
      "Epoch 0. Batches: 800/824. Loss: 7.679732799530029.\n",
      "Epoch 1. Batches: 0/824. Loss: 7.404466152191162.\n",
      "Epoch 1. Batches: 50/824. Loss: 7.644104480743408.\n",
      "Epoch 1. Batches: 100/824. Loss: 7.486452579498291.\n",
      "Epoch 1. Batches: 150/824. Loss: 7.605063438415527.\n",
      "Epoch 1. Batches: 200/824. Loss: 7.189799785614014.\n",
      "Epoch 1. Batches: 250/824. Loss: 7.229679584503174.\n",
      "Epoch 1. Batches: 300/824. Loss: 7.474671840667725.\n",
      "Epoch 1. Batches: 350/824. Loss: 7.613611221313477.\n",
      "Epoch 1. Batches: 400/824. Loss: 7.702889919281006.\n",
      "Epoch 1. Batches: 450/824. Loss: 7.330658435821533.\n",
      "Epoch 1. Batches: 500/824. Loss: 7.526511192321777.\n",
      "Epoch 1. Batches: 550/824. Loss: 7.242801666259766.\n",
      "Epoch 1. Batches: 600/824. Loss: 7.50385856628418.\n",
      "Epoch 1. Batches: 650/824. Loss: 7.381448268890381.\n",
      "Epoch 1. Batches: 700/824. Loss: 7.422384738922119.\n",
      "Epoch 1. Batches: 750/824. Loss: 7.60336971282959.\n",
      "Epoch 1. Batches: 800/824. Loss: 7.534899711608887.\n",
      "Epoch 2. Batches: 0/824. Loss: 7.449667930603027.\n",
      "Epoch 2. Batches: 50/824. Loss: 7.420285701751709.\n",
      "Epoch 2. Batches: 100/824. Loss: 7.164694309234619.\n",
      "Epoch 2. Batches: 150/824. Loss: 7.572296142578125.\n",
      "Epoch 2. Batches: 200/824. Loss: 7.366830348968506.\n",
      "Epoch 2. Batches: 250/824. Loss: 7.4902424812316895.\n",
      "Epoch 2. Batches: 300/824. Loss: 7.698855400085449.\n",
      "Epoch 2. Batches: 350/824. Loss: 7.549055099487305.\n",
      "Epoch 2. Batches: 400/824. Loss: 7.419053554534912.\n",
      "Epoch 2. Batches: 450/824. Loss: 7.079553604125977.\n",
      "Epoch 2. Batches: 500/824. Loss: 7.525847911834717.\n",
      "Epoch 2. Batches: 550/824. Loss: 7.410069465637207.\n",
      "Epoch 2. Batches: 600/824. Loss: 7.51750373840332.\n",
      "Epoch 2. Batches: 650/824. Loss: 7.396381378173828.\n",
      "Epoch 2. Batches: 700/824. Loss: 7.255932807922363.\n",
      "Epoch 2. Batches: 750/824. Loss: 7.592586040496826.\n",
      "Epoch 2. Batches: 800/824. Loss: 7.6365275382995605.\n",
      "Epoch 3. Batches: 0/824. Loss: 7.464594841003418.\n",
      "Epoch 3. Batches: 50/824. Loss: 7.5210137367248535.\n",
      "Epoch 3. Batches: 100/824. Loss: 7.162569046020508.\n",
      "Epoch 3. Batches: 150/824. Loss: 7.316423416137695.\n",
      "Epoch 3. Batches: 200/824. Loss: 7.318066596984863.\n",
      "Epoch 3. Batches: 250/824. Loss: 7.150693416595459.\n",
      "Epoch 3. Batches: 300/824. Loss: 7.240060806274414.\n",
      "Epoch 3. Batches: 350/824. Loss: 7.4287519454956055.\n",
      "Epoch 3. Batches: 400/824. Loss: 7.46003532409668.\n",
      "Epoch 3. Batches: 450/824. Loss: 7.384961128234863.\n",
      "Epoch 3. Batches: 500/824. Loss: 7.594051361083984.\n",
      "Epoch 3. Batches: 550/824. Loss: 7.189775466918945.\n",
      "Epoch 3. Batches: 600/824. Loss: 7.340938091278076.\n",
      "Epoch 3. Batches: 650/824. Loss: 7.295268535614014.\n",
      "Epoch 3. Batches: 700/824. Loss: 7.413783550262451.\n",
      "Epoch 3. Batches: 750/824. Loss: 7.643817901611328.\n",
      "Epoch 3. Batches: 800/824. Loss: 7.205899715423584.\n",
      "Epoch 4. Batches: 0/824. Loss: 7.479830265045166.\n",
      "Epoch 4. Batches: 50/824. Loss: 7.3537702560424805.\n",
      "Epoch 4. Batches: 100/824. Loss: 7.047689914703369.\n",
      "Epoch 4. Batches: 150/824. Loss: 7.547991752624512.\n",
      "Epoch 4. Batches: 200/824. Loss: 7.347126483917236.\n",
      "Epoch 4. Batches: 250/824. Loss: 7.256986618041992.\n",
      "Epoch 4. Batches: 300/824. Loss: 7.123339653015137.\n",
      "Epoch 4. Batches: 350/824. Loss: 7.105064868927002.\n",
      "Epoch 4. Batches: 400/824. Loss: 7.083166122436523.\n",
      "Epoch 4. Batches: 450/824. Loss: 7.455001354217529.\n",
      "Epoch 4. Batches: 500/824. Loss: 7.316730499267578.\n",
      "Epoch 4. Batches: 550/824. Loss: 7.17060661315918.\n",
      "Epoch 4. Batches: 600/824. Loss: 7.274240016937256.\n",
      "Epoch 4. Batches: 650/824. Loss: 6.996017932891846.\n",
      "Epoch 4. Batches: 700/824. Loss: 7.378017425537109.\n",
      "Epoch 4. Batches: 750/824. Loss: 7.185276985168457.\n",
      "Epoch 4. Batches: 800/824. Loss: 7.185158729553223.\n",
      "Epoch 5. Batches: 0/824. Loss: 7.125516891479492.\n",
      "Epoch 5. Batches: 50/824. Loss: 7.035217761993408.\n",
      "Epoch 5. Batches: 100/824. Loss: 6.981350898742676.\n",
      "Epoch 5. Batches: 150/824. Loss: 6.930905818939209.\n",
      "Epoch 5. Batches: 200/824. Loss: 7.129185199737549.\n",
      "Epoch 5. Batches: 250/824. Loss: 7.153631687164307.\n",
      "Epoch 5. Batches: 300/824. Loss: 6.863980770111084.\n",
      "Epoch 5. Batches: 350/824. Loss: 7.0922465324401855.\n",
      "Epoch 5. Batches: 400/824. Loss: 6.949765205383301.\n",
      "Epoch 5. Batches: 450/824. Loss: 7.292142391204834.\n",
      "Epoch 5. Batches: 500/824. Loss: 7.002800464630127.\n",
      "Epoch 5. Batches: 550/824. Loss: 7.037431716918945.\n",
      "Epoch 5. Batches: 600/824. Loss: 6.960810661315918.\n",
      "Epoch 5. Batches: 650/824. Loss: 7.1653008460998535.\n",
      "Epoch 5. Batches: 700/824. Loss: 7.113593101501465.\n",
      "Epoch 5. Batches: 750/824. Loss: 6.758676052093506.\n",
      "Epoch 5. Batches: 800/824. Loss: 7.268561840057373.\n",
      "Epoch 6. Batches: 0/824. Loss: 6.991214752197266.\n",
      "Epoch 6. Batches: 50/824. Loss: 6.7949538230896.\n",
      "Epoch 6. Batches: 100/824. Loss: 6.950498104095459.\n",
      "Epoch 6. Batches: 150/824. Loss: 7.086835861206055.\n",
      "Epoch 6. Batches: 200/824. Loss: 7.080602645874023.\n",
      "Epoch 6. Batches: 250/824. Loss: 6.940727710723877.\n",
      "Epoch 6. Batches: 300/824. Loss: 7.097619533538818.\n",
      "Epoch 6. Batches: 350/824. Loss: 7.142930507659912.\n",
      "Epoch 6. Batches: 400/824. Loss: 7.122628211975098.\n",
      "Epoch 6. Batches: 450/824. Loss: 6.9664411544799805.\n",
      "Epoch 6. Batches: 500/824. Loss: 6.842757701873779.\n",
      "Epoch 6. Batches: 550/824. Loss: 6.893938064575195.\n",
      "Epoch 6. Batches: 600/824. Loss: 6.5967559814453125.\n",
      "Epoch 6. Batches: 650/824. Loss: 6.979867458343506.\n",
      "Epoch 6. Batches: 700/824. Loss: 6.409862041473389.\n",
      "Epoch 6. Batches: 750/824. Loss: 6.964216232299805.\n",
      "Epoch 6. Batches: 800/824. Loss: 6.731390476226807.\n",
      "Epoch 7. Batches: 0/824. Loss: 6.705855846405029.\n",
      "Epoch 7. Batches: 50/824. Loss: 6.6774582862854.\n",
      "Epoch 7. Batches: 100/824. Loss: 6.925770282745361.\n",
      "Epoch 7. Batches: 150/824. Loss: 6.610952377319336.\n",
      "Epoch 7. Batches: 200/824. Loss: 6.728976249694824.\n",
      "Epoch 7. Batches: 250/824. Loss: 6.733253002166748.\n",
      "Epoch 7. Batches: 300/824. Loss: 6.985194206237793.\n",
      "Epoch 7. Batches: 350/824. Loss: 6.8867716789245605.\n",
      "Epoch 7. Batches: 400/824. Loss: 6.92586612701416.\n",
      "Epoch 7. Batches: 450/824. Loss: 6.812682628631592.\n",
      "Epoch 7. Batches: 500/824. Loss: 6.644275665283203.\n",
      "Epoch 7. Batches: 550/824. Loss: 6.935220718383789.\n",
      "Epoch 7. Batches: 600/824. Loss: 6.690593242645264.\n",
      "Epoch 7. Batches: 650/824. Loss: 6.991061210632324.\n",
      "Epoch 7. Batches: 700/824. Loss: 6.663010120391846.\n",
      "Epoch 7. Batches: 750/824. Loss: 7.106461048126221.\n",
      "Epoch 7. Batches: 800/824. Loss: 6.599895000457764.\n",
      "Epoch 8. Batches: 0/824. Loss: 6.315891742706299.\n",
      "Epoch 8. Batches: 50/824. Loss: 6.690142631530762.\n",
      "Epoch 8. Batches: 100/824. Loss: 6.672346591949463.\n",
      "Epoch 8. Batches: 150/824. Loss: 6.401266574859619.\n",
      "Epoch 8. Batches: 200/824. Loss: 6.3775529861450195.\n",
      "Epoch 8. Batches: 250/824. Loss: 6.598895072937012.\n",
      "Epoch 8. Batches: 300/824. Loss: 6.3342108726501465.\n",
      "Epoch 8. Batches: 350/824. Loss: 6.571104526519775.\n",
      "Epoch 8. Batches: 400/824. Loss: 6.410487651824951.\n",
      "Epoch 8. Batches: 450/824. Loss: 6.256017684936523.\n",
      "Epoch 8. Batches: 500/824. Loss: 6.501552581787109.\n",
      "Epoch 8. Batches: 550/824. Loss: 6.5627055168151855.\n",
      "Epoch 8. Batches: 600/824. Loss: 6.504359722137451.\n",
      "Epoch 8. Batches: 650/824. Loss: 6.723425388336182.\n",
      "Epoch 8. Batches: 700/824. Loss: 6.754874229431152.\n",
      "Epoch 8. Batches: 750/824. Loss: 6.673011302947998.\n",
      "Epoch 8. Batches: 800/824. Loss: 6.425800323486328.\n",
      "Epoch 9. Batches: 0/824. Loss: 6.410562515258789.\n",
      "Epoch 9. Batches: 50/824. Loss: 6.504891872406006.\n",
      "Epoch 9. Batches: 100/824. Loss: 6.412689208984375.\n",
      "Epoch 9. Batches: 150/824. Loss: 6.112616062164307.\n",
      "Epoch 9. Batches: 200/824. Loss: 6.230381488800049.\n",
      "Epoch 9. Batches: 250/824. Loss: 6.1721110343933105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9. Batches: 300/824. Loss: 6.543927192687988.\n",
      "Epoch 9. Batches: 350/824. Loss: 6.5060224533081055.\n",
      "Epoch 9. Batches: 400/824. Loss: 6.29942512512207.\n",
      "Epoch 9. Batches: 450/824. Loss: 6.613790512084961.\n",
      "Epoch 9. Batches: 500/824. Loss: 6.112674713134766.\n",
      "Epoch 9. Batches: 550/824. Loss: 6.004948616027832.\n",
      "Epoch 9. Batches: 600/824. Loss: 6.2785444259643555.\n",
      "Epoch 9. Batches: 650/824. Loss: 6.289528846740723.\n",
      "Epoch 9. Batches: 700/824. Loss: 6.222177028656006.\n",
      "Epoch 9. Batches: 750/824. Loss: 6.145449161529541.\n",
      "Epoch 9. Batches: 800/824. Loss: 6.04640007019043.\n",
      "Epoch 10. Batches: 0/824. Loss: 6.080854415893555.\n",
      "Epoch 10. Batches: 50/824. Loss: 6.383656024932861.\n",
      "Epoch 10. Batches: 100/824. Loss: 6.101265907287598.\n",
      "Epoch 10. Batches: 150/824. Loss: 6.27835750579834.\n",
      "Epoch 10. Batches: 200/824. Loss: 6.136828422546387.\n",
      "Epoch 10. Batches: 250/824. Loss: 6.277472496032715.\n",
      "Epoch 10. Batches: 300/824. Loss: 6.163389205932617.\n",
      "Epoch 10. Batches: 350/824. Loss: 6.014395236968994.\n",
      "Epoch 10. Batches: 400/824. Loss: 6.098059177398682.\n",
      "Epoch 10. Batches: 450/824. Loss: 6.2154622077941895.\n",
      "Epoch 10. Batches: 500/824. Loss: 6.155284404754639.\n",
      "Epoch 10. Batches: 550/824. Loss: 6.070863723754883.\n",
      "Epoch 10. Batches: 600/824. Loss: 6.327295303344727.\n",
      "Epoch 10. Batches: 650/824. Loss: 6.597641468048096.\n",
      "Epoch 10. Batches: 700/824. Loss: 6.227560520172119.\n",
      "Epoch 10. Batches: 750/824. Loss: 5.969326019287109.\n",
      "Epoch 10. Batches: 800/824. Loss: 6.212796688079834.\n",
      "Epoch 11. Batches: 0/824. Loss: 5.8886942863464355.\n",
      "Epoch 11. Batches: 50/824. Loss: 5.959369659423828.\n",
      "Epoch 11. Batches: 100/824. Loss: 5.967865467071533.\n",
      "Epoch 11. Batches: 150/824. Loss: 5.9161882400512695.\n",
      "Epoch 11. Batches: 200/824. Loss: 5.730714321136475.\n",
      "Epoch 11. Batches: 250/824. Loss: 5.917727470397949.\n",
      "Epoch 11. Batches: 300/824. Loss: 6.025115489959717.\n",
      "Epoch 11. Batches: 350/824. Loss: 6.138430595397949.\n",
      "Epoch 11. Batches: 400/824. Loss: 6.119592189788818.\n",
      "Epoch 11. Batches: 450/824. Loss: 6.192376613616943.\n",
      "Epoch 11. Batches: 500/824. Loss: 5.994596004486084.\n",
      "Epoch 11. Batches: 550/824. Loss: 5.6154656410217285.\n",
      "Epoch 11. Batches: 600/824. Loss: 5.646873950958252.\n",
      "Epoch 11. Batches: 650/824. Loss: 5.957241058349609.\n",
      "Epoch 11. Batches: 700/824. Loss: 5.8320698738098145.\n",
      "Epoch 11. Batches: 750/824. Loss: 5.691587448120117.\n",
      "Epoch 11. Batches: 800/824. Loss: 5.713565826416016.\n",
      "Epoch 12. Batches: 0/824. Loss: 5.581945419311523.\n",
      "Epoch 12. Batches: 50/824. Loss: 5.576005935668945.\n",
      "Epoch 12. Batches: 100/824. Loss: 5.6689453125.\n",
      "Epoch 12. Batches: 150/824. Loss: 5.61985969543457.\n",
      "Epoch 12. Batches: 200/824. Loss: 5.7849321365356445.\n",
      "Epoch 12. Batches: 250/824. Loss: 5.49661922454834.\n",
      "Epoch 12. Batches: 300/824. Loss: 5.632369518280029.\n",
      "Epoch 12. Batches: 350/824. Loss: 5.694591522216797.\n",
      "Epoch 12. Batches: 400/824. Loss: 5.807604789733887.\n",
      "Epoch 12. Batches: 450/824. Loss: 5.782433032989502.\n",
      "Epoch 12. Batches: 500/824. Loss: 5.835403919219971.\n",
      "Epoch 12. Batches: 550/824. Loss: 5.877771377563477.\n",
      "Epoch 12. Batches: 600/824. Loss: 5.754243850708008.\n",
      "Epoch 12. Batches: 650/824. Loss: 5.584428787231445.\n",
      "Epoch 12. Batches: 700/824. Loss: 5.809832572937012.\n",
      "Epoch 12. Batches: 750/824. Loss: 5.70475959777832.\n",
      "Epoch 12. Batches: 800/824. Loss: 5.674479961395264.\n",
      "Epoch 13. Batches: 0/824. Loss: 5.342681407928467.\n",
      "Epoch 13. Batches: 50/824. Loss: 5.695554733276367.\n",
      "Epoch 13. Batches: 100/824. Loss: 5.666298866271973.\n",
      "Epoch 13. Batches: 150/824. Loss: 5.132085800170898.\n",
      "Epoch 13. Batches: 200/824. Loss: 5.4908528327941895.\n",
      "Epoch 13. Batches: 250/824. Loss: 5.456081390380859.\n",
      "Epoch 13. Batches: 300/824. Loss: 5.106464862823486.\n",
      "Epoch 13. Batches: 350/824. Loss: 5.322142124176025.\n",
      "Epoch 13. Batches: 400/824. Loss: 5.219539165496826.\n",
      "Epoch 13. Batches: 450/824. Loss: 5.506075859069824.\n",
      "Epoch 13. Batches: 500/824. Loss: 5.658933162689209.\n",
      "Epoch 13. Batches: 550/824. Loss: 5.55928897857666.\n",
      "Epoch 13. Batches: 600/824. Loss: 5.450706958770752.\n",
      "Epoch 13. Batches: 650/824. Loss: 5.456545829772949.\n",
      "Epoch 13. Batches: 700/824. Loss: 5.8351545333862305.\n",
      "Epoch 13. Batches: 750/824. Loss: 5.5166850090026855.\n",
      "Epoch 13. Batches: 800/824. Loss: 5.436371326446533.\n",
      "Epoch 14. Batches: 0/824. Loss: 4.910689353942871.\n",
      "Epoch 14. Batches: 50/824. Loss: 5.375247955322266.\n",
      "Epoch 14. Batches: 100/824. Loss: 5.293210506439209.\n",
      "Epoch 14. Batches: 150/824. Loss: 5.264815807342529.\n",
      "Epoch 14. Batches: 200/824. Loss: 5.276440620422363.\n",
      "Epoch 14. Batches: 250/824. Loss: 5.124446868896484.\n",
      "Epoch 14. Batches: 300/824. Loss: 5.127643585205078.\n",
      "Epoch 14. Batches: 350/824. Loss: 5.25177001953125.\n",
      "Epoch 14. Batches: 400/824. Loss: 5.383098125457764.\n",
      "Epoch 14. Batches: 450/824. Loss: 4.928143501281738.\n",
      "Epoch 14. Batches: 500/824. Loss: 5.321273326873779.\n",
      "Epoch 14. Batches: 550/824. Loss: 5.4023823738098145.\n",
      "Epoch 14. Batches: 600/824. Loss: 5.247175693511963.\n",
      "Epoch 14. Batches: 650/824. Loss: 5.291445255279541.\n",
      "Epoch 14. Batches: 700/824. Loss: 5.642919540405273.\n",
      "Epoch 14. Batches: 750/824. Loss: 5.384143829345703.\n",
      "Epoch 14. Batches: 800/824. Loss: 5.105086803436279.\n",
      "Epoch 15. Batches: 0/824. Loss: 5.249324321746826.\n",
      "Epoch 15. Batches: 50/824. Loss: 5.232989311218262.\n",
      "Epoch 15. Batches: 100/824. Loss: 5.1960954666137695.\n",
      "Epoch 15. Batches: 150/824. Loss: 4.874007701873779.\n",
      "Epoch 15. Batches: 200/824. Loss: 5.219900608062744.\n",
      "Epoch 15. Batches: 250/824. Loss: 5.1294965744018555.\n",
      "Epoch 15. Batches: 300/824. Loss: 5.269804000854492.\n",
      "Epoch 15. Batches: 350/824. Loss: 5.064185619354248.\n",
      "Epoch 15. Batches: 400/824. Loss: 5.223998546600342.\n",
      "Epoch 15. Batches: 450/824. Loss: 5.208873748779297.\n",
      "Epoch 15. Batches: 500/824. Loss: 5.053745746612549.\n",
      "Epoch 15. Batches: 550/824. Loss: 5.382382392883301.\n",
      "Epoch 15. Batches: 600/824. Loss: 5.120967388153076.\n",
      "Epoch 15. Batches: 650/824. Loss: 5.350961208343506.\n",
      "Epoch 15. Batches: 700/824. Loss: 5.3405985832214355.\n",
      "Epoch 15. Batches: 750/824. Loss: 5.3020195960998535.\n",
      "Epoch 15. Batches: 800/824. Loss: 5.533185005187988.\n",
      "Epoch 16. Batches: 0/824. Loss: 5.058448791503906.\n",
      "Epoch 16. Batches: 50/824. Loss: 4.993682384490967.\n",
      "Epoch 16. Batches: 100/824. Loss: 5.127678871154785.\n",
      "Epoch 16. Batches: 150/824. Loss: 4.810416221618652.\n",
      "Epoch 16. Batches: 200/824. Loss: 5.117741584777832.\n",
      "Epoch 16. Batches: 250/824. Loss: 5.039691925048828.\n",
      "Epoch 16. Batches: 300/824. Loss: 5.4177021980285645.\n",
      "Epoch 16. Batches: 350/824. Loss: 5.038583755493164.\n",
      "Epoch 16. Batches: 400/824. Loss: 5.177677154541016.\n",
      "Epoch 16. Batches: 450/824. Loss: 5.115966796875.\n",
      "Epoch 16. Batches: 500/824. Loss: 5.295034408569336.\n",
      "Epoch 16. Batches: 550/824. Loss: 5.3231201171875.\n",
      "Epoch 16. Batches: 600/824. Loss: 5.1790666580200195.\n",
      "Epoch 16. Batches: 650/824. Loss: 5.110982894897461.\n",
      "Epoch 16. Batches: 700/824. Loss: 5.093069076538086.\n",
      "Epoch 16. Batches: 750/824. Loss: 5.229064464569092.\n",
      "Epoch 16. Batches: 800/824. Loss: 4.960470199584961.\n",
      "Epoch 17. Batches: 0/824. Loss: 4.937798500061035.\n",
      "Epoch 17. Batches: 50/824. Loss: 4.590719699859619.\n",
      "Epoch 17. Batches: 100/824. Loss: 4.833527088165283.\n",
      "Epoch 17. Batches: 150/824. Loss: 4.917508602142334.\n",
      "Epoch 17. Batches: 200/824. Loss: 5.148914813995361.\n",
      "Epoch 17. Batches: 250/824. Loss: 5.011996269226074.\n",
      "Epoch 17. Batches: 300/824. Loss: 5.13336181640625.\n",
      "Epoch 17. Batches: 350/824. Loss: 4.9976983070373535.\n",
      "Epoch 17. Batches: 400/824. Loss: 5.23707389831543.\n",
      "Epoch 17. Batches: 450/824. Loss: 5.341649055480957.\n",
      "Epoch 17. Batches: 500/824. Loss: 5.039352893829346.\n",
      "Epoch 17. Batches: 550/824. Loss: 5.199528217315674.\n",
      "Epoch 17. Batches: 600/824. Loss: 5.557004451751709.\n",
      "Epoch 17. Batches: 650/824. Loss: 4.942445278167725.\n",
      "Epoch 17. Batches: 700/824. Loss: 5.040471076965332.\n",
      "Epoch 17. Batches: 750/824. Loss: 4.729445457458496.\n",
      "Epoch 17. Batches: 800/824. Loss: 5.050894737243652.\n",
      "Epoch 18. Batches: 0/824. Loss: 5.041764736175537.\n",
      "Epoch 18. Batches: 50/824. Loss: 5.014035701751709.\n",
      "Epoch 18. Batches: 100/824. Loss: 4.813475608825684.\n",
      "Epoch 18. Batches: 150/824. Loss: 4.82271146774292.\n",
      "Epoch 18. Batches: 200/824. Loss: 4.612640857696533.\n",
      "Epoch 18. Batches: 250/824. Loss: 5.153663158416748.\n",
      "Epoch 18. Batches: 300/824. Loss: 4.929352760314941.\n",
      "Epoch 18. Batches: 350/824. Loss: 5.0709710121154785.\n",
      "Epoch 18. Batches: 400/824. Loss: 5.029210090637207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18. Batches: 450/824. Loss: 5.038995742797852.\n",
      "Epoch 18. Batches: 500/824. Loss: 4.880331516265869.\n",
      "Epoch 18. Batches: 550/824. Loss: 5.305757522583008.\n",
      "Epoch 18. Batches: 600/824. Loss: 4.792581081390381.\n",
      "Epoch 18. Batches: 650/824. Loss: 5.246639251708984.\n",
      "Epoch 18. Batches: 700/824. Loss: 4.685539722442627.\n",
      "Epoch 18. Batches: 750/824. Loss: 4.871416091918945.\n",
      "Epoch 18. Batches: 800/824. Loss: 5.005054950714111.\n",
      "Epoch 19. Batches: 0/824. Loss: 4.504660129547119.\n",
      "Epoch 19. Batches: 50/824. Loss: 4.640068054199219.\n",
      "Epoch 19. Batches: 100/824. Loss: 5.020140647888184.\n",
      "Epoch 19. Batches: 150/824. Loss: 5.262938976287842.\n",
      "Epoch 19. Batches: 200/824. Loss: 4.810288429260254.\n",
      "Epoch 19. Batches: 250/824. Loss: 5.2104716300964355.\n",
      "Epoch 19. Batches: 300/824. Loss: 4.875925540924072.\n",
      "Epoch 19. Batches: 350/824. Loss: 5.323032855987549.\n",
      "Epoch 19. Batches: 400/824. Loss: 5.025117874145508.\n",
      "Epoch 19. Batches: 450/824. Loss: 4.952408313751221.\n",
      "Epoch 19. Batches: 500/824. Loss: 4.819994926452637.\n",
      "Epoch 19. Batches: 550/824. Loss: 4.955278396606445.\n",
      "Epoch 19. Batches: 600/824. Loss: 5.212903022766113.\n",
      "Epoch 19. Batches: 650/824. Loss: 5.041627883911133.\n",
      "Epoch 19. Batches: 700/824. Loss: 4.717978477478027.\n",
      "Epoch 19. Batches: 750/824. Loss: 4.893381118774414.\n",
      "Epoch 19. Batches: 800/824. Loss: 4.978625297546387.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "losses = np.zeros(len(dataloader) * n_epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses[len(dataloader) * epoch + batch_idx] = loss.item()\n",
    "        \n",
    "        if not batch_idx % log_interval:\n",
    "            print(\"Epoch {}. Batches: {}/{}. Loss: {}.\".format(\n",
    "                epoch, batch_idx, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models parameters\n",
    "torch.save(model.state_dict(), \"./state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10268, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get weights of the first FC layer\n",
    "embeddings = F.relu(model.linear1.weight.data).T\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag-of-docs model\n",
    "texts = []\n",
    "for bookname in os.listdir(\"./books/\"):\n",
    "    with open(\"./books/\" + bookname) as inf:\n",
    "        text = \" \".join(inf.readlines())\n",
    "    texts.append(text)\n",
    "    \n",
    "cv = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "result = cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0, 37, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  2,  0],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.toarray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save words from the wordsim together with their similarity values\n",
    "a_words = list(chosen_words)\n",
    "valid_pairs = []\n",
    "\n",
    "with open(\"combined.csv\") as inf:\n",
    "    for line in inf.readlines():\n",
    "        ws = line.split(\",\")\n",
    "        if ws[0] in chosen_words and ws[1] in chosen_words:\n",
    "            valid_pairs.append((ws[0], ws[1], float(ws[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity of two words \n",
    "def similarity(embeddings, w_list, w1, w2):\n",
    "    idx1 = w_list.index(w1)\n",
    "    idx2 = w_list.index(w2)\n",
    "    \n",
    "    v1 = embeddings[idx1, :]\n",
    "    v2 = embeddings[idx2, :]\n",
    "    \n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_a = np.linalg.norm(v1)\n",
    "    norm_b = np.linalg.norm(v2)\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mse\n",
    "def calc_mse(embeddings, w_list, pairs):\n",
    "    mse = .0\n",
    "    for pair in pairs:\n",
    "        w1 = pair[0]\n",
    "        w2 = pair[1]\n",
    "        sim = pair[2]\n",
    "        sim_cal = similarity(embeddings, w_list, w1, w2)\n",
    "    \n",
    "        mse += (sim / 10. - sim_cal) ** 2\n",
    "    \n",
    "    mse /= len(valid_pairs)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V embedding mse: 0.09040535084477971\n",
      "BOW embedding mse: 0.12781699469779034\n"
     ]
    }
   ],
   "source": [
    "print(\"W2V embedding mse: {}\".format(calc_mse(embeddings, dataset.word_list, valid_pairs)))\n",
    "print(\"BOW embedding mse: {}\".format(calc_mse(result.toarray().T, cv.get_feature_names(), valid_pairs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
